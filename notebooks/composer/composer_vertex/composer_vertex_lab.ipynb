{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c97e967-ebe6-43dc-b6e3-952bafc144ed",
   "metadata": {},
   "source": [
    "# A Simple Composer Pipeline using Vertex AI Training, Model Upload and Model Deployment\n",
    "**Learning Objectives:**\n",
    "  1. Create custom Airflow operators that leverage Vertex AI jobs with docker containers.\n",
    "  2. Build an Airflow pipeline that uses Vertex AI for training, model upload, and deployment.\n",
    "  3. Run the Airflow pipeline with Cloud Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641a95a-c00a-40f3-b557-bc733b9ae345",
   "metadata": {},
   "source": [
    "### Before Starting: Spin up Composer Environment. \n",
    "This will take approximately 25 minutes. \n",
    "\n",
    "**TODO**: Click the plus in the upper left and open a terminal. Copy and run the following command without change to create a Composer environment in your Google Cloud project: \n",
    "\n",
    "```gcloud composer environments create my-composer-env --location=us-central1 --image-version=composer-1.17.8-airflow-2.1.4```\n",
    "\n",
    "After you run the command, feel free to return to this notebook and continue working through steps while your Composer environment spins up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8524b1b-9153-42f6-b600-00ce80a1207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "PROJECT = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "COMPOSER_ENV = \"my-composer-env\"\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"COMPOSER_ENV\"] = COMPOSER_ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e5d10-9bf0-449c-9036-120f2f14b9bd",
   "metadata": {},
   "source": [
    "## Organize/Clean the Dataset\n",
    "* Create a BigQuery Dataset\n",
    "* Query a public dataset to create tables of clean data for training and testing\n",
    "\n",
    "In this lab you will be working with the babyweight dataset.\n",
    "\n",
    "Step 1: Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b1ab6-d191-4fc7-9180-9ff4354c6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create a BigQuery dataset for babyweight if it doesn't exist\n",
    "datasetexists=$(bq ls -d | grep -w babyweight)\n",
    "\n",
    "if [ -n \"$datasetexists\" ]; then\n",
    "    echo -e \"BigQuery dataset already exists, let's not recreate it.\"\n",
    "\n",
    "else\n",
    "    echo \"Creating BigQuery dataset titled: babyweight\"\n",
    "    \n",
    "    bq --location=US mk --dataset \\\n",
    "        --description \"Babyweight\" \\\n",
    "        $PROJECT:babyweight\n",
    "    echo \"Here are your current datasets:\"\n",
    "    bq ls\n",
    "fi\n",
    "    \n",
    "## Create GCS bucket if it doesn't exist already...\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "    echo -e \"Bucket exists, let's not recreate it.\"\n",
    "    \n",
    "else\n",
    "    echo \"Creating a new GCS bucket.\"\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    echo \"Here are your current buckets:\"\n",
    "    gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61947b-210e-4765-957d-293ef78a59fc",
   "metadata": {},
   "source": [
    "Step 2: Create training and eval tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31b681-9aed-4f66-9962-7d46f5aa1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    CAST(is_male AS STRING) AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = 1 THEN \"Single(1)\"\n",
    "        WHEN plurality = 2 THEN \"Twins(2)\"\n",
    "        WHEN plurality = 3 THEN \"Triplets(3)\"\n",
    "        WHEN plurality = 4 THEN \"Quadruplets(4)\"\n",
    "        WHEN plurality = 5 THEN \"Quintuplets(5)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    FARM_FINGERPRINT(\n",
    "        CONCAT(\n",
    "            CAST(year AS STRING),\n",
    "            CAST(month AS STRING)\n",
    "        )\n",
    "    ) AS hashmonth\n",
    "FROM\n",
    "    publicdata.samples.natality\n",
    "WHERE\n",
    "    year > 2000\n",
    "    AND weight_pounds > 0\n",
    "    AND mother_age > 0\n",
    "    AND plurality > 0\n",
    "    AND gestation_weeks > 0;\n",
    "    \n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_augmented_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data\n",
    "UNION ALL\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    \"Unknown\" AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = \"Single(1)\" THEN plurality\n",
    "        ELSE \"Multiple(2+)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data;\n",
    "    \n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_train AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) < 3;\n",
    "    \n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_eval AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde1e0f-5990-4911-9e3b-d3fd82f4b63f",
   "metadata": {},
   "source": [
    "### Training Application \n",
    "* In babyweight/trainer, feel free to look at `model.py` and `task.py`. These files contain code for a Tensorflow training application that builds and trains a model to predict baby weight.  \n",
    "* Running the next two cells will package the Tensorflow training application as a source distribution and upload a gzipped tar file of the application to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b8685-3cb4-4903-86b8-fedac4e91bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='babyweight_trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Babyweight model training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44d6f8-9a11-43e3-ab45-ec0b561b8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd babyweight\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd ..\n",
    "gsutil cp babyweight/dist/babyweight_trainer-0.1.tar.gz gs://${BUCKET}/babyweight/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b4df2-7a20-49e1-b975-9ba379c904b3",
   "metadata": {},
   "source": [
    "### Custom Airflow Operators for Vertex AI  \n",
    "* Airflow doesnt currently have Vertex AI operators, so we will build and push docker containers that leverage the Vertex API, so we can use Vertex services in our Composer DAG \n",
    "\n",
    "Feel free to look inside the folders vertex_train_docker, vertex_upload_model_docker, and vertex_deploy_docker if you are interested in the implementation of these custom containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76d93d-e47a-4934-9da2-cd8c562697ab",
   "metadata": {},
   "source": [
    "Here we are creating Cloud Build config files to build and push the containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e4403-8127-4a69-8b9f-b20b9da7b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "vertex_train_cloudbuild = {\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\"build\", \"-t\", f\"gcr.io/{PROJECT}/vertex_train_image:latest\", \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\"push\", f\"gcr.io/{PROJECT}/vertex_train_image:latest\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "vertex_upload_cloudbuild = {\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\n",
    "                \"build\",\n",
    "                \"-t\",\n",
    "                f\"gcr.io/{PROJECT}/vertex_upload_model_image:latest\",\n",
    "                \".\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\"push\", f\"gcr.io/{PROJECT}/vertex_upload_model_image:latest\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "vertex_deploy_cloudbuild = {\n",
    "    \"steps\": [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\n",
    "                \"build\",\n",
    "                \"-t\",\n",
    "                f\"gcr.io/{PROJECT}/vertex_deploy_image:latest\",\n",
    "                \".\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": [\"push\", f\"gcr.io/{PROJECT}/vertex_deploy_image:latest\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"./vertex_train_docker/cloudbuild.json\", \"w\") as outfile:\n",
    "    json.dump(vertex_train_cloudbuild, outfile)\n",
    "    \n",
    "with open(\"./vertex_upload_model_docker/cloudbuild.json\", \"w\") as outfile:\n",
    "    json.dump(vertex_upload_cloudbuild, outfile)\n",
    "    \n",
    "with open(\"./vertex_deploy_docker/cloudbuild.json\", \"w\") as outfile:\n",
    "    json.dump(vertex_deploy_cloudbuild, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebd5f6-847b-4865-a5fc-1684d2c703bf",
   "metadata": {},
   "source": [
    "#### Build and push the containers to your projects private Container Regsitry. \n",
    "Later in the lab, you will use the Image URI of these containers to instatiate them as operators in your Airflow DAG. Each of these 3 cells may take a few minutes to run, as Docker containers are being built and pushed to your projects private Container Registry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6760e6-5d29-4eb5-abe9-67e65103d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd vertex_upload_model_docker\n",
    "chmod +x build_image.sh\n",
    "./build_image.sh\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1ae66-8092-48fb-995c-01170fb9dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd vertex_train_docker\n",
    "chmod +x build_image.sh\n",
    "./build_image.sh\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b22994-d3c0-4290-99b9-511a6ee0ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd vertex_deploy_docker\n",
    "chmod +x build_image.sh\n",
    "./build_image.sh\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e3bb7-d93f-49a7-9038-37c4c99f74a5",
   "metadata": {},
   "source": [
    "Now you can build out your Composer DAG. The steps of the DAG will be:\n",
    "* Export the data from your training and eval BigQuery tables to sharded CSVs in GCS\n",
    "* Launch a Vertex AI Custom Training Job to train the Tensorflow model\n",
    "* Upload the model to Vertex AI\n",
    "* Create an endpoint and deploy the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8b05d-7ef8-4e2a-9483-261d48eafffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight_composer_dag.py\n",
    "import datetime\n",
    "import logging\n",
    "from base64 import b64encode as b64e\n",
    "from airflow import DAG\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator\n",
    "from airflow.contrib.operators.bigquery_to_gcs import (\n",
    "    BigQueryToCloudStorageOperator)\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    'owner': 'Google Cloud Learner',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime.datetime.now(),\n",
    "    'email': ['gcp.learning@fake-email.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "def _get_project_id():\n",
    "    \"\"\"Get project ID from default Google Cloud connection.\"\"\"\n",
    "\n",
    "    extras = BaseHook.get_connection(\"google_cloud_default\").extra_dejson\n",
    "    key = \"extra__google_cloud_platform__project\"\n",
    "    if key in extras:\n",
    "        project_id = extras[key]\n",
    "    else:\n",
    "        raise (\"Must configure project_id in google_cloud_default \"\n",
    "               \"connection from Airflow Console\")\n",
    "    return project_id\n",
    "\n",
    "PROJECT = _get_project_id()\n",
    "BUCKET = PROJECT\n",
    "\n",
    "# Output to store the model\n",
    "OUTDIR= f'gs://{BUCKET}/babyweight/trained_model'\n",
    "DATETIME = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# BQ Dataset and train/eval table names\n",
    "DATASET = 'babyweight'\n",
    "TRAIN_TABLE = 'babyweight_data_train'\n",
    "EVAL_TABLE = 'babyweight_data_eval'\n",
    "\n",
    "# Define Airflow DAG\n",
    "with DAG(\n",
    "        'babyweight_dag',\n",
    "        catchup=False,\n",
    "        default_args=DEFAULT_ARGS) as dag:\n",
    "    \n",
    "    # File path to store sharded CSVs exported from BigQuery\n",
    "    data_path = f\"gs://{BUCKET}/babyweight/data/\"\n",
    "    \n",
    "    # Operator to shard the training data table to CSVs in GCS\n",
    "    bq_export_train_csv_op = BigQueryToCloudStorageOperator(\n",
    "        task_id=\"bq_export_gcs_train_csv_task\",\n",
    "        source_project_dataset_table=f\"{DATASET}.{TRAIN_TABLE}\",\n",
    "        destination_cloud_storage_uris=[data_path + \"train-*.csv\"],\n",
    "        export_format=\"CSV\",\n",
    "        print_header=False,\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Operator to shard the eval data table to CSVs in GCS\n",
    "    bq_export_eval_csv_op = BigQueryToCloudStorageOperator(\n",
    "        task_id=\"bq_export_gcs_eval_csv_task\",\n",
    "        source_project_dataset_table=f\"{DATASET}.{EVAL_TABLE}\",\n",
    "        destination_cloud_storage_uris=[data_path + \"eval-*.csv\"],\n",
    "        export_format=\"CSV\",\n",
    "        print_header=False,\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Operator to launch a Vertex AI Custom Training job\n",
    "    vertex_train_op = (\n",
    "        KubernetesPodOperator(\n",
    "            image=f\"gcr.io/{PROJECT}/vertex_train_image:latest\",\n",
    "            name=\"vertex_train_pod\",\n",
    "            arguments=[\n",
    "                '--ml_framework=tensorflow',\n",
    "                f'--project={PROJECT}',\n",
    "                '--region=us-central1',\n",
    "                '--job_display_name=babyweight-model-{}'.format(DATETIME),\n",
    "                '--replica_count=1',\n",
    "                '--pre_built_training_container_uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest',\n",
    "                f'--model_package_gcs_path=gs://{BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz',\n",
    "                '--python_module=trainer.task',\n",
    "                '--machine_type=n1-standard-4',\n",
    "                f'--trainer_args={{\"train_data_path\": \"gs://{BUCKET}/babyweight/data/train*.csv\", \"eval_data_path\": \"gs://{BUCKET}/babyweight/data/eval*.csv\", \"output_dir\": \"{OUTDIR}\", \"num_epochs\": 10, \"train_examples\": 10000, \"eval_steps\": 100, \"batch_size\": 32, \"nembeds\": 8}}'\n",
    "            ],\n",
    "            namespace=\"default\",\n",
    "            task_id=\"vertex_train_task\",\n",
    "            dag=dag\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Operator to upload model to Vertex\n",
    "    vertex_upload_model_op = (\n",
    "        KubernetesPodOperator(\n",
    "            image=f\"gcr.io/{PROJECT}/vertex_upload_model_image:latest\",\n",
    "            name=\"vertex_upload_model_pod\",\n",
    "            arguments=[\n",
    "                '--ml_framework=tensorflow',\n",
    "                f'--project={PROJECT}',\n",
    "                '--region=us-central1',\n",
    "                '--model_display_name=babyweight-model-{}'.format(DATETIME),\n",
    "                '--serving_container_image_uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest',\n",
    "                f'--artifact_uri={OUTDIR}',\n",
    "            ],\n",
    "            namespace=\"default\",\n",
    "            task_id=\"vertex_upload_model_task\",\n",
    "            dag=dag\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    # Operator to create endpoint and deploy model \n",
    "    vertex_deploy_op = (\n",
    "        KubernetesPodOperator(\n",
    "            image=f\"gcr.io/{PROJECT}/vertex_deploy_image:latest\",\n",
    "            name=\"vertex_deploy_pod\",\n",
    "            arguments=[\n",
    "                f'--project={PROJECT}',\n",
    "                '--region=us-central1',\n",
    "                '--endpoint_display_name=babyweight-composer-model-endpoint',\n",
    "                '--model_display_name=babyweight-model-{}'.format(DATETIME),\n",
    "                '--deployed_model_display_name=deployed_model',\n",
    "                '--machine_type=n1-standard-4',\n",
    "            ],\n",
    "            namespace=\"default\",\n",
    "            task_id=\"vertex_deploy_task\",\n",
    "            dag=dag\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    [bq_export_train_csv_op, bq_export_eval_csv_op] >> vertex_train_op\n",
    "    vertex_train_op >> vertex_upload_model_op\n",
    "    vertex_upload_model_op >> vertex_deploy_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda29ef-886f-492f-88aa-fd2524599261",
   "metadata": {},
   "source": [
    "Upload the DAG to your Composer environment. \n",
    "\n",
    "**Note**: If you get an error here, your Composer environment is likely still spinning up. You can verify this by navigating to the Composer UI in Google Cloud Console. You will need to wait for your Composer environment to be created before moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80536718-2034-4bab-a14f-4174d563de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud composer environments storage dags import \\\n",
    "    --environment my-composer-env1  \\\n",
    "    --location $REGION \\\n",
    "    --source babyweight_composer_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1e566-1f5a-4fed-8d01-9a5b77faec36",
   "metadata": {},
   "source": [
    "Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3e6f2-cc5c-465b-a5a9-500533de98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud composer environments run $COMPOSER_ENV \\\n",
    "    --location $REGION \\\n",
    "    dags trigger -- babyweight_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a585b1-27e9-46f2-99b0-d08e688ef137",
   "metadata": {},
   "source": [
    "Monitor your pipeline run in the Airflow UI\n",
    "* Run the following command to output the config of your Composer environment.\n",
    "* Click on the airflowUri link to launch the Airflow UI where you can view and monitor your pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee13cc-0db0-4a08-b862-7aa3ebbc7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud composer environments describe $COMPOSER_ENV --location $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26757d-5cdc-4860-ae76-25e2e56a5389",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "When you are finished with the lab spin down your Cloud Composer environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90f601-d6a4-4acb-8cdf-d7d5bb7e8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud composer environments delete $COMPOSER_ENV \\\n",
    "    --location $REGION"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
